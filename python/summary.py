# -*- coding: utf-8 -*-
"""
Multi-modal Video Summarization (TH output, transcript-first)
Workflow:
  1) (optional) Download audio + extract frames by scene-cut
  2) ASR (Whisper) -> transcription.txt
  3) Image captioning (Florence-2 by default) + optional OCR -> captions.json
  4) Merge by timestamp -> scene-level "facts" (for visual notes)
  5) Global summaries:
     - final_summary_transcript.txt              (TRANSCRIPT only)
     - final_summary_transcript_plus_visual.txt  (TRANSCRIPT primary + VISUAL evidence)
---------------------------------------------------------------
Reqs:
  - ffmpeg, ffprobe
  - pip install yt_dlp openai-whisper torch torchvision torchaudio
  - pip install transformers accelerate pillow
"""

import os, sys, functools
os.environ["HF_ATTENTION_BACKEND"] = "PYTORCH_EAGER"
os.environ["FLASH_ATTENTION_FORCE_DISABLE"] = "1"
os.environ["TRANSFORMERS_NO_ADVISORY_WARNINGS"] = "1"
os.environ["PYTORCH_ENABLE_FLASH_SDP"] = "0"

class _StderrLogger:
    def debug(self, msg):
        # ‡πÄ‡∏á‡∏µ‡∏¢‡∏ö debug (‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏∞ log ‡πÑ‡∏õ stderr ‡∏Å‡πá‡πÑ‡∏î‡πâ)
        pass
    def warning(self, msg):
        print(f"‚ö†Ô∏è {msg}", file=sys.stderr, flush=True)
    def error(self, msg):
        print(f"‚ùå {msg}", file=sys.stderr, flush=True)

import io, json, time, base64, shutil, subprocess, re, math
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional
from pythainlp.tokenize import word_tokenize

import requests
import torch
from PIL import Image
import whisper  # openai-whisper
import textwrap

from datetime import datetime

_progress_fp = None
try:
    _progress_fp = os.fdopen(3, "w", buffering=1, encoding="utf-8")  # line-buffered
except Exception:
    _progress_fp = None

def send_progress(step: str, percent: int, subprogress: int):
    if _progress_fp:
        _progress_fp.write(json.dumps({"type":"progress","step":step,"percent":percent,"subprogress": subprogress}) + "\n")
        _progress_fp.flush()
    # ‡πÑ‡∏°‡πà‡πÅ‡∏ï‡∏∞ stdout ‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î
    
if sys.platform.startswith('win'):
    try:
        sys.stdout.reconfigure(encoding='utf-8', errors='replace')
        sys.stderr.reconfigure(encoding='utf-8', errors='replace')
    except Exception:
        pass
    os.environ.setdefault('PYTHONIOENCODING', 'utf-8')

# ====== CONFIG ======
YOUTUBE_URL = "https://youtu.be/Rq7plosixd0?si=-Xw05o5mTZd-eTBt"
AUDIO_OUT = "audio.wav"
FRAMES_DIR = "frames"
SCENES_JSON = "scenes.json"
CAPTIONS_JSON = "captions.json"
SCENE_FACTS_JSON = "scene_facts.json"
TRANSCRIPT_TXT = "transcription.txt"
TRANSCRIPT_SEGMENTS = "transcript_segments.json"
METRICS_JSON = globals().get("METRICS_JSON", None)
log = functools.partial(print, file=sys.stderr, flush=True)

LANGUAGE = "th"
WHISPER_MODEL = "large-v3-turbo"
WHISPER_TEMP = float(os.environ.get("WHISPER_TEMP", "0.0"))  # Whisper temperature (0.0 = deterministic)
ASR_DEVICE = "cpu"      
VL_DEVICE  = "cuda"     # ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö Florence ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô

VL_MODEL_NAME = "microsoft/Florence-2-base"
SCENE_THRESH = 0.6
ENABLE_OCR = False
USE_YOUTUBE_TRANSCRIPT = True  # ‡πÉ‡∏ä‡πâ youtube_transcript_api ‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏£‡∏Å (‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ Whisper ‡∏°‡∏≤‡∏Å)

# ‡πÉ‡∏ä‡πâ 127.0.0.1 ‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤ IPv6/localhost ‡∏ö‡∏≤‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á
OLLAMA_API = os.environ.get("OLLAMA_API", "http://127.0.0.1:11434/api/chat")
OLLAMA_MODEL = os.environ.get("OLLAMA_MODEL", "llama3:8b")  

# ===== NEW OUTPUT NAMES =====
DROPDOWN_JSON = "dropdown_items.json"           # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö UI dropdown
FINAL_TXT = "dropdown_list.txt"  # bullet ‡∏£‡∏ß‡∏°
FINAL_ARTICLE_TXT = "final_article_th.txt"      # ‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡πâ‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢

# ===== STRONG THAI-ONLY SYSTEM (‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏°‡πÅ‡∏ö‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏Å‡πâ‡∏ß‡πÑ‡∏ó‡∏¢) =====
SYSTEM_PROMPT_TH = "‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô"

# ===== NEW: Generation presets (minimal-safe) =====
GEN_OPTS_QUALITY = {
    "temperature": 0.3,        # ‡πÉ‡∏Å‡∏•‡πâ default ‡∏Ç‡∏≠‡∏á Ollama
    "top_p": 0.9,
    "top_k": 40,
    "repeat_penalty": 1.15,    # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 1.6 -> 1.15 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏õ‡∏£‡∏∞‡∏´‡∏•‡∏≤‡∏î/‡∏ã‡πâ‡∏≥
    "repeat_last_n": 256,      # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏±‡∏ô‡∏ß‡∏ô
    "num_ctx": 8192,           # llama3 8B ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö ~8k
    "num_predict": 1024,       # ‡πÄ‡∏û‡∏î‡∏≤‡∏ô‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡πâ‡∏ô
    "stop": ["<|eot_id|>", "</s>"],  # stop ‡∏Ç‡∏≠‡∏á Llama 3
}
GEN_OPTS_FAST = {
    "temperature": 0.6,
    "top_p": 0.9,
    "top_k": 40,
    "repeat_penalty": 1.12,
    "repeat_last_n": 256,
    "num_ctx": 8192,
    "num_predict": 512,
    "stop": ["<|eot_id|>", "</s>"],
}

# ===== NEW: Word count helpers (TH/EN mix safe) =====
WS_SPLIT_RE = re.compile(r"[ \t\r\n]+")
def word_count_th(text: str) -> int:
    t = (text or "").strip()
    if not t:
        return 0
    tokens = [tok for tok in WS_SPLIT_RE.split(t) if tok]
    if len(tokens) >= 50:
        return len(tokens)
    approx = max(len(t) // 5, len(tokens))
    return approx

def clamp_article_to_words(text: str, min_words: int, max_words: int) -> str:
    t = (text or "").strip()
    for _ in range(2):
        wc = word_count_th(t)
        if min_words <= wc <= max_words:
            return t
        if wc < min_words:
            prompt = (
                f"‡∏Ç‡∏¢‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÉ‡∏´‡πâ‡∏¢‡∏≤‡∏ß‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì {min_words}-{max_words} ‡∏Ñ‡∏≥ "
                f"‡∏Ñ‡∏á‡∏™‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏•‡∏∞‡∏™‡∏≤‡∏£‡∏∞‡πÄ‡∏î‡∏¥‡∏° ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏™‡πà‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏¢‡πà‡∏≠‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏•‡∏Ç‡∏•‡∏¥‡∏™‡∏ï‡πå:\n{t}"
            )
            t = ensure_thai(ollama_summarize(prompt, options=GEN_OPTS_QUALITY))
        else:
            prompt = (
                f"‡∏¢‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÉ‡∏´‡πâ‡∏¢‡∏≤‡∏ß‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì {min_words}-{max_words} ‡∏Ñ‡∏≥ "
                f"‡∏Ñ‡∏á‡∏™‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏•‡∏∞‡∏™‡∏≤‡∏£‡∏∞‡πÄ‡∏î‡∏¥‡∏° ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏¢‡πà‡∏≠‡∏¢/‡πÄ‡∏•‡∏Ç‡∏•‡∏¥‡∏™‡∏ï‡πå:\n{t}"
            )
            t = ensure_thai(ollama_summarize(prompt, options=GEN_OPTS_QUALITY))
    return t

# ====== UTIL ======
def wrap_text(text: str, width: int = 100) -> str:
    """
    ‡∏à‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô width ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ï‡πà‡∏≠‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
    """
    # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô list ‡∏Ç‡∏≠‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡πÅ‡∏•‡πâ‡∏ß
    wrapped_lines = textwrap.wrap(text, width=width, replace_whitespace=True, drop_whitespace=True)
    # ‡∏£‡∏ß‡∏°‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà ‡πÇ‡∏î‡∏¢‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢ '\n'
    return "\n".join(wrapped_lines)

def check_cmd(cmd: str):
    if shutil.which(cmd) is None:
        raise RuntimeError(f"‚ùå '{cmd}' not found in PATH.")

def run(cmd: List[str]):
    p = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    if p.returncode != 0:
        raise RuntimeError(f"Command failed: {' '.join(cmd)}\nSTDERR:\n{p.stderr}")
    return p.stdout

def image_to_b64(img: Image.Image) -> str:
    buf = io.BytesIO()
    img.save(buf, format="JPEG")
    return base64.b64encode(buf.getvalue()).decode("utf-8")

THAI_RE = re.compile(r"[‡∏Å-‡πô]")
def looks_thai(s: str) -> bool:
    return bool(THAI_RE.search(s or ""))

# ===== NEW: sanitize options + healthcheck + fallback /api/chat =====
ALLOWED_OLLAMA_KEYS = {
    "temperature", "top_p", "top_k",
    "repeat_penalty", "repeat_last_n",
    "num_ctx", "num_predict",
    "stop", "seed",
}

def sanitize_ollama_options(opts: Optional[Dict[str, Any]]) -> Dict[str, Any]:
    if not opts:
        return {}
    return {k: v for k, v in opts.items() if k in ALLOWED_OLLAMA_KEYS}

def ollama_healthcheck(base: str) -> bool:
    url = base.rsplit("/", 1)[0] + "/tags"
    try:
        r = requests.get(url, timeout=5)
        return r.ok
    except Exception:
        return False

def ollama_ensure_model(model: str, base: str) -> None:
    url = base.rsplit("/", 1)[0] + "/pull"
    try:
        requests.post(url, json={"name": model}, timeout=60)
    except Exception:
        pass

def _post_json(url: str, payload: dict, stream: bool, timeout: int):
    last_exc = None
    for _ in range(2):
        try:
            resp = requests.post(url, json=payload, stream=stream, timeout=timeout)
            if resp.status_code == 405 and url.endswith("/generate"):
                return resp
            resp.raise_for_status()
            return resp
        except Exception as e:
            last_exc = e
            stream = False
    if last_exc:
        raise last_exc

def ollama_summarize(
    prompt: str,
    options: Optional[Dict[str, Any]] = None,
    system: Optional[str] = None,
    stream: bool = False,           # ‡∏õ‡∏¥‡∏î‡∏™‡∏ï‡∏£‡∏µ‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏£‡∏ö
    timeout: int = 600,
) -> str:
    base = OLLAMA_API
    if system is None:
        system = SYSTEM_PROMPT_TH
    if not ollama_healthcheck(base):
        raise RuntimeError("‚ùå ‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠ Ollama ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ `ollama serve` ‡∏£‡∏±‡∏ô‡∏≠‡∏¢‡∏π‡πà ‡πÅ‡∏•‡∏∞‡∏û‡∏≠‡∏£‡πå‡∏ï 11434 ‡πÄ‡∏õ‡∏¥‡∏î‡∏≠‡∏¢‡∏π‡πà")

    ollama_ensure_model(OLLAMA_MODEL, base)

    payload = {
        "model": OLLAMA_MODEL,
        "stream": False,
        "messages": [
            {"role": "system", "content": system},
            {"role": "user", "content": prompt},
        ],
    }

    # Ollama API expects generation options in the "options" key, not at payload level
    if options:
        # Filter only allowed Ollama generation options
        ALLOWED_OPTS = {
            "temperature", "top_p", "top_k", "repeat_penalty", "repeat_last_n",
            "num_ctx", "num_predict", "stop", "seed"
        }
        filtered_opts = {k: v for k, v in options.items() if k in ALLOWED_OPTS}
        if filtered_opts:
            payload["options"] = filtered_opts

    resp = requests.post(base, json=payload, timeout=timeout)
    resp.raise_for_status()
    data = resp.json()
    msg = (data.get("message") or {}).get("content", "")
    return (msg or "").strip()

def ensure_thai(text: str, max_chars: int = None) -> str:
    t = (text or "").strip()
    if looks_thai(t):
        if max_chars and len(t) > max_chars:
            t = t[:max_chars]
        return t
    p1 = (
        "‡πÅ‡∏õ‡∏•/‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô '‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏•‡πâ‡∏ß‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô' "
        "‡∏´‡πâ‡∏≤‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ô‡∏≥ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°:\n"
        f"{t}"
    )
    out = (ollama_summarize(p1, options=GEN_OPTS_FAST) or "").strip()
    if not looks_thai(out):
        p2 = (
            "‡∏à‡∏á‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô '‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏•‡πâ‡∏ß‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô' ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡πÅ‡∏°‡πâ‡πÅ‡∏ï‡πà‡∏Ñ‡∏≥‡πÄ‡∏î‡∏µ‡∏¢‡∏ß "
            "‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏™‡πà‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ Here is, Translation, ‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏≥‡∏≠‡∏∑‡πà‡∏ô ‡πÜ "
            "‡πÉ‡∏´‡πâ‡∏™‡πà‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô:\n"
            f"{t}"
        )
        out = (ollama_summarize(p2, options=GEN_OPTS_FAST) or "").strip()
    if max_chars and len(out) > max_chars:
        out = out[:max_chars]
    return out

# ===== NEW: Thai sentence tools & de-dup =====
SENT_SPLIT_RE = re.compile(r"(?:\s*(?<=[\.!?‚Ä¶])\s+|\n+)")
ELLIPSIS_RE = re.compile(r"(\.{2,}|‚Ä¶{2,})")
MULTI_SPACE_RE = re.compile(r"\s+")
COMMON_FIXES = {
    "‡∏ö‡∏£‡∏¥‡∏©‡∏≤‡∏™": "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó",
    "‡∏î‡∏±‡∏ä‡∏ô‡∏¥": "‡∏î‡∏±‡∏ä‡∏ô‡∏µ",
    "‡∏ó‡∏£‡∏∏": "‡∏ó‡∏∞‡∏•‡∏∏",
    "‡∏û‡∏¥‡∏™‡∏∏‡∏î": "‡∏û‡∏¥‡∏™‡∏π‡∏à‡∏ô‡πå",
    "‡∏ï‡∏ö‡πÅ‡∏ó‡∏ô": "‡∏ï‡∏≠‡∏ö‡πÅ‡∏ó‡∏ô",
    "‡∏Å‡∏∞‡∏à‡∏∏‡∏Å‡∏ï‡∏±‡∏ß": "‡∏Å‡∏£‡∏∞‡∏à‡∏∏‡∏Å‡∏ï‡∏±‡∏ß",
    "‡∏•‡∏≠‡∏á‡∏ó‡∏∏‡∏ô": "‡∏•‡∏á‡∏ó‡∏∏‡∏ô",
}

def split_sentences_th(text: str) -> List[str]:
    t = text.strip()
    t = ELLIPSIS_RE.sub("‚Ä¶", t)
    parts = [p.strip() for p in SENT_SPLIT_RE.split(t) if p.strip()]
    return parts

def simple_tokenize_th(s: str) -> List[str]:
    s = re.sub(r"[^\w‡∏Å-‡πô%\.:\-/]", " ", s, flags=re.UNICODE)
    s = re.sub(r"\s+", " ", s).strip().lower()
    return s.split()

def jaccard_sim(a: str, b: str) -> float:
    A, B = set(simple_tokenize_th(a)), set(simple_tokenize_th(b))
    if not A or not B:
        from difflib import SequenceMatcher
        return SequenceMatcher(None, a, b).ratio()
    return len(A & B) / max(1, len(A | B))

def similarity(a: str, b: str) -> float:
    return jaccard_sim(a, b)

def dedup_sentences(text: str, thr: float = 0.88) -> str:
    sents = split_sentences_th(text)
    keep = []
    for s in sents:
        s_norm = MULTI_SPACE_RE.sub(" ", s)
        if not s_norm:
            continue
        if any(similarity(s_norm, k) >= thr or s_norm == k for k in keep):
            continue
        keep.append(s_norm)
    out = " ".join(keep)
    return MULTI_SPACE_RE.sub(" ", out).strip()

def basic_thai_fixes(text: str) -> str:
    out = text
    for wrong, right in COMMON_FIXES.items():
        out = out.replace(wrong, right)
    out = re.sub(r"(‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏™‡∏µ‡πÅ‡∏î‡∏á)+", "‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏™‡∏µ‡πÅ‡∏î‡∏á", out)
    out = re.sub(r"(‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡πÉ‡∏´‡∏ç‡πà\s*10\s*‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó)+", "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡πÉ‡∏´‡∏ç‡πà 10 ‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó", out)
    return out

def enforce_paragraphs(text: str, min_para=2, max_para=4) -> str:
    t = MULTI_SPACE_RE.sub(" ", text).strip()
    words = t.split(" ")
    if len(words) < 120:
        return t
    chunks = []
    target = max(min_para, min(max_para, 3))
    step = max(1, len(words)//target)
    for i in range(target):
        start = i*step
        end = None if i == target-1 else (i+1)*step
        para = " ".join(words[start:end]).strip()
        if para:
            chunks.append(para)
    return "\n\n".join(chunks)

def polish_thai_article(text: str, min_words=300, max_words=400) -> str:
    t = basic_thai_fixes(text)
    t = dedup_sentences(t, thr=0.88)
    prompt = ("‡∏õ‡∏£‡∏±‡∏ö‡∏™‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ó‡∏¢‡∏Ç‡∏≠‡∏á‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏•‡∏∑‡πà‡∏ô‡πÑ‡∏´‡∏• ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ "
              "‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏™‡∏≤‡∏£‡∏∞‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç ‡∏´‡πâ‡∏≤‡∏°‡∏ó‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏•‡∏¥‡∏™‡∏ï‡πå ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠:\n" + t)
    t2 = ensure_thai(ollama_summarize(prompt, options=GEN_OPTS_FAST))
    t2 = dedup_sentences(t2, thr=0.9)
    t2 = clamp_article_to_words(t2, min_words, max_words)
    t2 = enforce_paragraphs(t2, 2, 4)
    return t2

def needs_retry(text: str, min_words=300, max_words=400) -> bool:
    wc = word_count_th(text)
    if wc < min_words*0.9 or wc > max_words*1.1:
        return True
    sents = split_sentences_th(text)
    dup_count = 0
    for i in range(1, len(sents)):
        if similarity(sents[i], sents[i-1]) >= 0.92:
            dup_count += 1
    return dup_count >= 1

def normalize_transcript_for_summary(t: str) -> str:
    p = f"‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏∞‡∏Å‡∏î ‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ ‡πÅ‡∏•‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡πÇ‡∏î‡∏¢‡∏Ñ‡∏á‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°:\n{t}"
    cleaned = ollama_summarize(p, options=GEN_OPTS_FAST)
    return ensure_thai(cleaned)

def delete_all_files_in_directory(directory_path):
    """
    Deletes all files within a specified directory.

    Args:
        directory_path (str): The path to the directory.
    """
    if not os.path.isdir(directory_path):
        log(f"Error: Directory '{directory_path}' does not exist.")
        return

    for filename in os.listdir(directory_path):
        file_path = os.path.join(directory_path, filename)
        if os.path.isfile(file_path):  # Ensure it's a file, not a subdirectory
            try:
                os.remove(file_path)
                log(f"Deleted: {file_path}")
            except OSError as e:
                log(f"Error deleting {file_path}: {e}")

# get duration in sec for DB
def get_video_duration(video_path: str) -> float:
    """‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ (float)"""
    try:
        cmd = [
            "ffprobe", "-v", "error",
            "-show_entries", "format=duration",
            "-of", "json",
            video_path
        ]
        result = subprocess.run(cmd, capture_output=True, text=True)
        data = json.loads(result.stdout)
        duration = float(data["format"]["duration"])
        return duration
    except Exception as e:
        log(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡πÑ‡∏î‡πâ: {e}")
        return 0.0

# ====== STEP 1: Download audio + scene-cut frames ======
COOKIES_FILE = os.environ.get("YDL_COOKIES")
BROWSER_FOR_COOKIES = os.environ.get("YDL_BROWSER", "chrome")
YDL_RETRIES = 3

def ydl_opts_common(outtmpl: str = "%(title).200B.%(ext)s"):
    opts = {
        "quiet": True,
        "no_warnings": True,
        "logger": _StderrLogger(),
        "noprogress": True,
        "noplaylist": True,
        "retries": YDL_RETRIES,
        "fragment_retries": YDL_RETRIES,
        "concurrent_fragment_downloads": 4,
        "throttledratelimit": 0,
        "geo_bypass": True,
        "nocheckcertificate": True,
        "http_headers": {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                          "AppleWebKit/537.36 (KHTML, like Gecko) "
                          "Chrome/120.0.0.0 Safari/537.36",
            "Accept-Language": "th,en-US;q=0.9,en;q=0.8",
        },
        "extractor_args": {
            "youtube": {
                "player_client": ["android", "web"],
            }
        },
        "outtmpl": outtmpl,
    }
    if COOKIES_FILE and os.path.exists(COOKIES_FILE):
        opts["cookiefile"] = COOKIES_FILE
    return opts

# ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å youtube ‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô .m4a ‡πÅ‡∏•‡πâ‡∏ß‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô .wav ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ whisper ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ(‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ö‡∏ö bestaudio)
def download_audio_wav_16k(url: str, out_path: str):
    import yt_dlp
    tmp_in = None
    opts = ydl_opts_common(outtmpl="tmp_audio_raw.%(ext)s")
    opts.update({
        "format": "bestaudio/best",
        "keepvideo": False,
        "merge_output_format": "m4a",
    })
    last_err = None
    for attempt in range(1, YDL_RETRIES + 1):
        try:
            with yt_dlp.YoutubeDL(opts) as ydl:
                info = ydl.extract_info(url, download=True)
                tmp_in = ydl.prepare_filename(info)
            break
        except Exception as e:
            last_err = e
            log(f"‚ö†Ô∏è audio download attempt {attempt}/{YDL_RETRIES} failed: {e}")
            opts["extractor_args"]["youtube"]["player_client"] = ["web", "android"]
    if not tmp_in or not os.path.exists(tmp_in):
        raise RuntimeError(f"‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (403/‡∏ö‡∏•‡πá‡∏≠‡∏Ñ): {last_err}")
    run(["ffmpeg", "-y", "-i", tmp_in, "-ar", "16000", "-ac", "1", out_path])
    try: os.remove(tmp_in)
    except: pass
    log(f"‚úÖ Audio saved -> {out_path}")

# ‡πÇ‡∏´‡∏•‡∏î‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏à‡∏≤‡∏Å youtube ‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô .mp4 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö scene-cut(‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ö‡∏ö bv*+ba/b)
def download_video_file(url: str) -> str:
    import yt_dlp
    outtmpl = "tmp_video.%(ext)s"
    opts = ydl_opts_common(outtmpl=outtmpl)
    opts.update({
        "format": "bv*+ba/b",
        "merge_output_format": "mp4",
    })
    last_err = None
    for attempt in range(1, YDL_RETRIES + 1):
        try:
            with yt_dlp.YoutubeDL(opts) as ydl:
                ydl.download([url])
            break
        except Exception as e:
            last_err = e
            log(f"‚ö†Ô∏è video download attempt {attempt}/{YDL_RETRIES} failed: {e}")
            opts["extractor_args"]["youtube"]["player_client"] = ["web", "android"]
    candidates = [f for f in os.listdir(".") if f.startswith("tmp_video.") and
                  f.lower().endswith((".mp4", ".mkv", ".webm"))]
    if not candidates:
        raise RuntimeError(f"‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (403/‡∏ö‡∏•‡πá‡∏≠‡∏Ñ): {last_err}")
    return sorted(candidates, key=os.path.getsize, reverse=True)[0]

# ====== STEP 2: ASR (Whisper) ======
def transcribe_whisper(
    wav_path: str,
    model_name: str,
    language: str,
    device: str,
    step_start: int = 10,
    step_end: int = 45,
) -> tuple[str, List[Dict[str, Any]]]:
    """
    ‡∏ñ‡∏≠‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏î‡πâ‡∏ß‡∏¢ whisper ‡πÅ‡∏•‡∏∞ return segments ‡∏û‡∏£‡πâ‡∏≠‡∏° timestamps
    Returns:
        tuple: (full_text, segments)
        - full_text: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        - segments: list of {start, end, text} ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡∏Å‡∏±‡∏ö visual
    """

    log("üîÑ Loading Whisper model...")
    model = whisper.load_model(model_name, device=device)
    
    if _progress_fp:
        _progress_fp.write(json.dumps({"type":"model_loaded"}) + "\n")
        _progress_fp.flush()
    
    send_progress("‡∏ñ‡∏≠‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á", step_start, 0)
    log("‚úÖ Model loaded, starting transcription...")

    result = model.transcribe(
        wav_path,
        language=language,
        fp16=(device == "cuda"),
        temperature=WHISPER_TEMP,
        condition_on_previous_text=True,
        initial_prompt=None,
        compression_ratio_threshold=None,
        verbose=False,
    )

    # ‡∏î‡∏∂‡∏á segments ‡∏û‡∏£‡πâ‡∏≠‡∏° timestamps
    segments = []
    for seg in result.get("segments", []):
        segments.append({
            "start": seg["start"],
            "end": seg["end"],
            "text": (seg.get("text") or "").strip()
        })

    text = (result["text"] or "").strip()
    text = ensure_thai(text)

    with open(TRANSCRIPT_TXT, "w", encoding="utf-8") as f:
        f.write(text)

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å segments ‡πÅ‡∏¢‡∏Å‡∏î‡πâ‡∏ß‡∏¢
    with open(TRANSCRIPT_SEGMENTS, "w", encoding="utf-8") as f:
        json.dump(segments, f, ensure_ascii=False, indent=2)
    log(f"‚úÖ Saved {len(segments)} transcript segments")

    send_progress("‡∏ñ‡∏≠‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á", step_end, 100)
    log("‚úÖ Transcription done.")

    return text, segments


# ====== STEP 2B: YouTube Transcript API (‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà 2 - ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ Whisper ‡∏°‡∏≤‡∏Å) ======
def extract_video_id(url: str) -> str:
    """‡∏î‡∏∂‡∏á video ID ‡∏à‡∏≤‡∏Å YouTube URL"""
    import re
    patterns = [
        r'(?:v=|/v/|youtu\.be/|/embed/|/shorts/)([a-zA-Z0-9_-]{11})',
        r'^([a-zA-Z0-9_-]{11})$'
    ]
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    return ""

def transcribe_youtube_api(
    youtube_url: str,
    language: str = "th",
    step_start: int = 10,
    step_end: int = 45,
) -> tuple[str, List[Dict[str, Any]]]:
    """
    ‡∏î‡∏∂‡∏á transcript ‡∏à‡∏≤‡∏Å YouTube ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ youtube_transcript_api
    ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ Whisper ‡∏°‡∏≤‡∏Å‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î audio ‡πÅ‡∏•‡∏∞ process
    
    Returns:
        tuple: (full_text, segments)
        - full_text: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        - segments: list of {start, end, text} ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡∏Å‡∏±‡∏ö visual
    """
    try:
        from youtube_transcript_api import YouTubeTranscriptApi
    except ImportError:
        raise RuntimeError("‚ùå youtube_transcript_api ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á: pip install youtube-transcript-api")
    
    video_id = extract_video_id(youtube_url)
    if not video_id:
        raise ValueError(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á video ID ‡∏à‡∏≤‡∏Å URL: {youtube_url}")
    
    log(f"üîÑ Fetching YouTube transcript for video: {video_id}")
    send_progress("‡∏ñ‡∏≠‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á", step_start, 0)
    
    try:
        # ‡πÉ‡∏ä‡πâ API format ‡πÉ‡∏´‡∏°‡πà (version 1.x)
        ytt_api = YouTubeTranscriptApi()
        
        # ‡∏•‡∏≠‡∏á‡∏î‡∏∂‡∏á transcript - ‡∏à‡∏∞‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏´‡∏≤ transcript ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
        # ‡∏•‡∏≠‡∏á‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß fallback ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏∑‡πà‡∏ô
        transcript_data = None
        transcript_type = ""
        
        try:
            # ‡∏•‡∏≠‡∏á‡∏î‡∏∂‡∏á‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏Å‡πà‡∏≠‡∏ô
            transcript_data = ytt_api.fetch(video_id, languages=[language, 'th'])
            transcript_type = f"thai ({language})"
            log(f"‚úÖ ‡∏û‡∏ö transcript ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢")
        except Exception as e:
            log(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö transcript ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢: {e}")
            
            # ‡∏•‡∏≠‡∏á‡∏î‡∏∂‡∏á‡∏†‡∏≤‡∏©‡∏≤‡πÉ‡∏î‡∏Å‡πá‡πÑ‡∏î‡πâ
            try:
                transcript_data = ytt_api.fetch(video_id)
                transcript_type = "auto"
                log(f"‚úÖ ‡∏û‡∏ö transcript (auto)")
            except Exception as e2:
                raise RuntimeError(f"‡πÑ‡∏°‡πà‡∏û‡∏ö transcript: {e2}")
        
        send_progress("‡∏ñ‡∏≠‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á", (step_start + step_end) // 2, 50)
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô format ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö Whisper
        # ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡∏Å‡∏≥‡∏´‡∏ô‡∏î end = start ‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
        raw_items = []
        
        # transcript_data ‡πÄ‡∏õ‡πá‡∏ô FetchedTranscript object, iterate ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢
        for item in transcript_data:
            start = item.start
            text = (item.text or '').strip()
            if text:
                raw_items.append({"start": start, "text": text})
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á segments ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ start ‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô end
        segments = []
        full_text_parts = []
        for i, item in enumerate(raw_items):
            if i < len(raw_items) - 1:
                end_time = raw_items[i + 1]["start"]
            else:
                # segment ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ - ‡πÉ‡∏ä‡πâ start + duration ‡πÇ‡∏î‡∏¢‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì (5 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)
                end_time = item["start"] + 5.0
            
            segments.append({
                "start": item["start"],
                "end": end_time,
                "text": item["text"]
            })
            full_text_parts.append(item["text"])
        
        full_text = " ".join(full_text_parts)
        
        # ‡∏ñ‡πâ‡∏≤ transcript ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡πÉ‡∏´‡πâ‡πÅ‡∏õ‡∏•
        if not looks_thai(full_text) and len(full_text) > 50:
            log("üîÑ ‡πÅ‡∏õ‡∏• transcript ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢...")
            full_text = ensure_thai(full_text)
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå
        with open(TRANSCRIPT_TXT, "w", encoding="utf-8") as f:
            f.write(full_text)
        
        with open(TRANSCRIPT_SEGMENTS, "w", encoding="utf-8") as f:
            json.dump(segments, f, ensure_ascii=False, indent=2)
        
        send_progress("‡∏ñ‡∏≠‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á", step_end, 100)
        log(f"‚úÖ YouTube Transcript done: {len(segments)} segments ({transcript_type})")
        
        return full_text, segments
        
    except Exception as e:
        raise RuntimeError(f"‚ùå YouTube Transcript API error: {e}")


# ====== STEP 3: Image Captioning (+ optional OCR) ======
def translate_to_th(text: str, max_chars: int = 200) -> str:
    if not text or looks_thai(text): return text or ""
    prompt = f"‡πÅ‡∏õ‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏ö‡∏ö‡∏™‡∏±‡πâ‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô {max_chars} ‡∏≠‡∏±‡∏Å‡∏Ç‡∏£‡∏∞:\n{text}"
    out = ollama_summarize(prompt, options=GEN_OPTS_FAST)
    return (out or "").strip()

class VisionCaptioner:
    def __init__(self, model_name: str, device: str):
        import os, torch
        from transformers import AutoProcessor, AutoModelForCausalLM

        # ‡∏Å‡∏±‡∏ô TF ‡πÇ‡∏ú‡∏•‡πà‡∏°‡∏≤‡∏Å‡∏ß‡∏ô
        os.environ["TRANSFORMERS_NO_TF"] = "1"
        os.environ["TRANSFORMERS_NO_FLAX"] = "1"
        # ‡∏õ‡∏¥‡∏î‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á SDPA/Flash ‡∏ó‡∏±‡πâ‡∏á‡∏£‡∏∞‡∏ö‡∏ö (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ö‡∏ô‡∏ö‡∏≤‡∏á‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô)
        os.environ["PYTORCH_ENABLE_FLASH_SDP"] = "0"
        os.environ["PYTORCH_ENABLE_MEM_EFFICIENT_SDP"] = "0"
        os.environ["PYTORCH_FORCE_DISABLE_FUSED_ADAM"] = "1"

        self.device = "cuda" if (device == "cuda" and torch.cuda.is_available()) else "cpu"
        self.backend = "florence"
        self.img_size = 448
        self.batch_size = 2
        self.max_new_tokens = 64

        self.processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            trust_remote_code=True,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            device_map="auto" if self.device == "cuda" else None,
            attn_implementation="eager",   # ‚¨ÖÔ∏è ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö eager
        )
        if self.device != "cuda":
            self.model.to("cpu")

        # --- ‡∏Å‡∏±‡∏ô‡∏û‡∏•‡∏≤‡∏î‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á SDPA/_supports_sdpa/use_cache ‡∏ó‡∏∏‡∏Å‡∏ä‡∏±‡πâ‡∏ô ---
        try: self.model.eval()
        except: pass

        for obj in [self.model,
                    getattr(self.model, "language_model", None),
                    getattr(self.model, "model", None),
                    getattr(self.model, "vision_tower", None)]:
            if obj is None: 
                continue
            # ‡∏õ‡∏¥‡∏î cache
            try: obj.config.use_cache = False
            except: pass
            try: obj.generation_config.use_cache = False
            except: pass
            # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö eager
            try: obj.config._attn_implementation = "eager"
            except: pass
            # ‡∏Å‡∏±‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏ù‡∏±‡πà‡∏á transformers ‡∏ó‡∏µ‡πà‡πÄ‡∏ä‡πá‡∏Ñ field ‡∏ô‡∏µ‡πâ
            if not hasattr(obj, "_supports_sdpa"):
                try: setattr(obj, "_supports_sdpa", False)
                except: pass

        log(f"‚úÖ Florence-2 ready on {self.device} (eager attention, no cache)")

    @torch.inference_mode()
    def _florence_generate(self, imgs, task: str):
        if not isinstance(imgs, list):
            imgs = [imgs]
        with torch.autocast("cuda", enabled=self.device == "cuda"):
            inputs = self.processor(text=[task]*len(imgs), images=imgs,
                                    return_tensors="pt", padding=True).to(self.device)
            ids = self.model.generate(
                **inputs,
                max_new_tokens=self.max_new_tokens,
                do_sample=False,
                num_beams=1,
                early_stopping=True,
                use_cache=False, 
            )
        outs = []
        for i in range(len(imgs)):
            try:
                txt = self.processor.batch_decode(ids[i:i+1], skip_special_tokens=False)[0]
                post = self.processor.post_process_generation(txt, task=task,
                                                              image_size=(imgs[i].height, imgs[i].width))
                val = post.get(task) or post.get("description") or txt
                outs.append(str(val).strip())
            except Exception:
                outs.append(self.processor.batch_decode(ids[i:i+1], skip_special_tokens=True)[0].strip())
        return outs

    def caption_image(self, img: Image.Image) -> Dict[str, Any]:
        if self.backend != "florence":
            txt = "‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏†‡∏≤‡∏û‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÅ‡∏ö‡∏ö‡∏™‡∏±‡πâ‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö"
            inputs = self.processor(images=img.convert("RGB"), text=txt, return_tensors="pt").to(self.device)
            out_ids = self.model.generate(**inputs, max_new_tokens=90, do_sample=False, num_beams=1)
            text = self.processor.decode(out_ids[0], skip_special_tokens=True)
            if not looks_thai(text):
                text = translate_to_th(text, max_chars=180)
            return {"caption_short": text[:80], "caption_detailed": text, "tags": []}

        # Florence: caption ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (<CAPTION>) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
        imgs = [img.convert("RGB").resize((448, 448))]
        caps = self._florence_generate(imgs, "<CAPTION>")
        cap = caps[0] if caps else ""
        cap = cap.replace("Caption the image", "").replace("Describe", "").strip()
        if not looks_thai(cap):
            cap = translate_to_th(cap, max_chars=90)
        return {"caption_short": cap, "caption_detailed": cap, "tags": []}

    def run_ocr(self, img: Image.Image) -> str:
        return ""
# download scene frames and caption them
def stream_scene_frames_and_caption(url: str,
                                    frames_dir: str,
                                    thresh: float,
                                    out_json: str,
                                    captioner: VisionCaptioner,
                                    video_duration: float | None = None,):
    os.makedirs(frames_dir, exist_ok=True)
    delete_all_files_in_directory(frames_dir)

    # ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
    video_path = download_video_file(url)
    duration = get_video_duration(video_path)

    def _safe_unlink(p: str):
        try:
            if p and os.path.exists(p):
                os.remove(p)
                log(f"üßπ removed temp file: {p}")
        except Exception as e:
            log(f"‚ö†Ô∏è failed to remove temp file {p}: {e}")

    results = []
    next_id = 1
    proc = None
    processed = 0
    estimated = 50
    if not isinstance(video_duration, (int, float)) or video_duration <= 0:
        video_duration = None
    try:
        # ‡∏£‡∏±‡∏ô ffmpeg ‡πÅ‡∏ï‡∏Å‡πÄ‡∏ü‡∏£‡∏°‡∏â‡∏≤‡∏Å + showinfo (stderr)
        cmd = [
            "ffmpeg", "-hide_banner", "-loglevel", "info",
            "-i", video_path,
            "-vf", f"select='gt(scene,{thresh})',showinfo",
            "-vsync", "vfr",
            os.path.join(frames_dir, "scene_%06d.jpg")
        ]
        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

        while True:
            line = proc.stderr.readline()
            if not line and proc.poll() is not None:
                break
            if not line:
                continue

            if "showinfo" in line and "pts_time:" in line:
                # ‡∏î‡∏∂‡∏á timestamp
                try:
                    ts = float(line.split("pts_time:")[1].split(" ")[0])
                except Exception:
                    ts = None

                # ‡∏£‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏ü‡∏•‡πå‡∏†‡∏≤‡∏û‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à
                img_path = os.path.join(frames_dir, f"scene_{next_id:06d}.jpg")
                for _ in range(50):
                    if os.path.exists(img_path) and os.path.getsize(img_path) > 0:
                        break
                    time.sleep(0.02)

                if os.path.exists(img_path):
                    try:
                        img = Image.open(img_path).convert("RGB")
                    except Exception:
                        time.sleep(0.05)
                        img = Image.open(img_path).convert("RGB")

                    cap = captioner.caption_image(img)
                    results.append({
                        "ts": round(ts, 2) if ts is not None else None,
                        "frame": os.path.basename(img_path),
                        "caption_short": cap.get("caption_short", ""),
                        "caption_detailed": cap.get("caption_detailed", ""),
                        "tags": cap.get("tags", []),
                        "ocr_text": ""
                    })
                    log(f"üñºÔ∏è {os.path.basename(img_path)} @{ts:.2f}s -> captioned")
                    processed += 1
                    if video_duration is not None and ts is not None:
                        ratio = max(0.0, min(ts / video_duration, 1.0))
                        subprogress = int(ratio * 100)
                    else:
                        subprogress = min(100, int(processed / estimated * 100))
                    percent = 45 + int((35 * subprogress) / 100)

                    send_progress("‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡∏†‡∏≤‡∏û", percent, subprogress)  # 45‚Äì80%

                    # ‡∏•‡∏ö‡∏†‡∏≤‡∏û‡πÄ‡∏ü‡∏£‡∏°‡∏ó‡∏±‡∏ô‡∏ó‡∏µ ‡∏•‡∏î IO/‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà
                    try: os.remove(img_path)
                    except: pass

                    next_id += 1

        # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ú‡∏• caption ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        with open(out_json, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        log(f"‚úÖ Captions saved -> {out_json}")

    finally:
        # ‡∏£‡∏≠ ffmpeg ‡∏õ‡∏¥‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡∏Å‡πà‡∏≠‡∏ô‡∏Ñ‡πà‡∏≠‡∏¢‡∏•‡∏ö‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß (‡∏Å‡∏±‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏•‡πá‡∏≠‡∏Å)
        try:
            if proc is not None:
                proc.wait(timeout=5)
        except Exception:
            pass
        _safe_unlink(video_path)
        return duration

# ====== STEP 4: Merge into scene-level facts ======
@dataclass
class TranscriptSegment:
    """Segment ‡∏à‡∏≤‡∏Å Whisper ‡∏û‡∏£‡πâ‡∏≠‡∏° timestamp"""
    start: float
    end: float
    text: str

@dataclass
class SceneFacts:
    start: float
    end: float
    speech: str
    visual_caption: str
    ocr_text: str
    tags: List[str]
# ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà transcript segments ‡∏Å‡∏±‡∏ö scene timestamps
def split_segments_to_scenes(
    segments: List[Dict[str, Any]],
    scene_ts: List[float]
) -> List[SceneFacts]:
    """
    ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà transcript segments ‡∏Å‡∏±‡∏ö scene timestamps ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ timestamp ‡∏à‡∏£‡∏¥‡∏á
    ‡πÅ‡∏ï‡πà‡∏•‡∏∞ scene ‡∏à‡∏∞‡∏°‡∏µ speech ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏±‡πâ‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÜ
    
    ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ scene_ts (ffmpeg ‡∏ï‡∏£‡∏ß‡∏à‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ scene cuts):
    - ‡πÉ‡∏ä‡πâ transcript segments ‡∏™‡∏£‡πâ‡∏≤‡∏á scene boundaries ‡πÅ‡∏ó‡∏ô
    - ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 15-30 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
    """
    if not segments:
        return [SceneFacts(0.0, 99999.0, "", "", "", [])]
    
    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ scene_ts -> ‡πÉ‡∏ä‡πâ transcript segments ‡∏™‡∏£‡πâ‡∏≤‡∏á scene boundaries
    if not scene_ts:
        log("‚ö†Ô∏è No scene cuts detected, using transcript segments as boundaries")
        
        # ‡∏Å‡∏•‡∏∏‡πà‡∏° segments ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏•‡∏∞ ~15-30 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
        SCENE_DURATION = 20.0  # ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
        facts: List[SceneFacts] = []
        
        current_start = segments[0].get("start", 0)
        current_texts = []
        
        for seg in segments:
            seg_end = seg.get("end", 0)
            seg_text = seg.get("text", "").strip()
            
            # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô ‡πÉ‡∏´‡πâ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
            if seg_end - current_start < SCENE_DURATION:
                current_texts.append(seg_text)
            else:
                # ‡∏à‡∏ö‡∏ä‡πà‡∏ß‡∏á‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô ‡∏™‡∏£‡πâ‡∏≤‡∏á SceneFacts
                if current_texts:
                    facts.append(SceneFacts(
                        start=current_start,
                        end=seg.get("start", current_start + SCENE_DURATION),
                        speech=" ".join(current_texts),
                        visual_caption="",
                        ocr_text="",
                        tags=[]
                    ))
                # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ä‡πà‡∏ß‡∏á‡πÉ‡∏´‡∏°‡πà
                current_start = seg.get("start", 0)
                current_texts = [seg_text]
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° scene ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
        if current_texts:
            last_end = segments[-1].get("end", current_start + SCENE_DURATION)
            facts.append(SceneFacts(
                start=current_start,
                end=last_end,
                speech=" ".join(current_texts),
                visual_caption="",
                ocr_text="",
                tags=[]
            ))
        
        log(f"‚úÖ Created {len(facts)} scenes from transcript segments")
        return facts if facts else [SceneFacts(0.0, 99999.0, " ".join(s.get("text", "") for s in segments), "", "", [])]
    
    scene_ts = sorted(scene_ts)
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á bounds: [(start1, end1), (start2, end2), ...]
    bounds = []
    for i, ts in enumerate(scene_ts):
        if i < len(scene_ts) - 1:
            bounds.append((ts, scene_ts[i+1]))
        else:
            bounds.append((ts, ts + 99999))
    
    facts: List[SceneFacts] = []
    for start, end in bounds:
        # ‡∏´‡∏≤ segments ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏µ‡πâ
        matching_segs = []
        for seg in segments:
            seg_start = seg.get("start", 0)
            seg_end = seg.get("end", 0)
            seg_mid = (seg_start + seg_end) / 2
            # ‡πÉ‡∏ä‡πâ‡∏à‡∏∏‡∏î‡∏Å‡∏∂‡πà‡∏á‡∏Å‡∏•‡∏≤‡∏á‡∏Ç‡∏≠‡∏á segment ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡∏ß‡πà‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô scene ‡πÑ‡∏´‡∏ô
            if start <= seg_mid < end:
                matching_segs.append(seg.get("text", "").strip())
        
        speech = " ".join(matching_segs)
        facts.append(SceneFacts(
            start=start,
            end=end,
            speech=speech,
            visual_caption="",
            ocr_text="",
            tags=[]
        ))
    
    return facts
# ‡∏ï‡∏£‡∏ß‡∏à‡∏ß‡πà‡∏≤ visual caption ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö speech ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà/‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå
def check_visual_relevance(speech: str, visual_caption: str) -> bool:
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏ß‡πà‡∏≤ visual caption ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö speech ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ True ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏ß‡∏£ merge, False ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£
    
    ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô:
    1. ‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡∏ó‡∏±‡∏ö‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç (‡πÑ‡∏°‡πà‡∏ô‡∏±‡∏ö stopwords)
    2. ‡∏´‡∏£‡∏∑‡∏≠ visual ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå (‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç, %, ‡πÄ‡∏ß‡∏•‡∏≤, ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞)
    """
    if not speech or not visual_caption:
        return False
    
    # Stopwords ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢/‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏© ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡∏ô‡∏±‡∏ö
    STOPWORDS = {
        "‡∏ó‡∏µ‡πà", "‡πÉ‡∏ô", "‡∏Ç‡∏≠‡∏á", "‡πÅ‡∏•‡∏∞", "‡πÄ‡∏õ‡πá‡∏ô", "‡∏°‡∏µ", "‡πÑ‡∏î‡πâ", "‡πÉ‡∏´‡πâ", "‡∏Å‡∏±‡∏ö", "‡∏à‡∏≤‡∏Å", "‡πÑ‡∏õ", "‡∏°‡∏≤", "‡∏≠‡∏¢‡∏π‡πà", "‡πÅ‡∏•‡πâ‡∏ß",
        "‡∏ô‡∏µ‡πâ", "‡∏ô‡∏±‡πâ‡∏ô", "‡∏Å‡πá", "‡∏à‡∏∞", "‡∏ß‡πà‡∏≤", "‡πÑ‡∏°‡πà", "‡πÄ‡∏£‡∏≤", "‡πÄ‡∏Ç‡∏≤", "‡∏Ñ‡∏∏‡∏ì", "‡∏ú‡∏°", "‡∏â‡∏±‡∏ô", "‡∏Ñ‡∏£‡∏±‡∏ö", "‡∏Ñ‡πà‡∏∞",
        "the", "a", "an", "is", "are", "was", "were", "on", "in", "at", "to", "for", "of", "with"
    }
    
    # Tokenize ‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á (‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢)
    def tokenize(text: str) -> set:
        from pythainlp.tokenize import word_tokenize 
        tokens = word_tokenize(text, engine="newmm")
        return {t.lower() for t in tokens if t not in STOPWORDS and len(t) > 1}
    
    speech_tokens = tokenize(speech)
    visual_tokens = tokenize(visual_caption)
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô‡∏ó‡∏±‡∏ö
    overlap = speech_tokens & visual_tokens
    if len(overlap) >= 2:  # ‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 2 ‡∏Ñ‡∏≥
        return True
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏ß‡πà‡∏≤ visual ‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå (‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç, %, ‡πÄ‡∏ß‡∏•‡∏≤)
    has_useful_data = bool(
        RE_NUMBER.search(visual_caption) or
        RE_PERCENT.search(visual_caption) or
        RE_TIMECODE.search(visual_caption) or
        RE_DATE.search(visual_caption) or
        RE_CURRENCY.search(visual_caption)
    )
    
    if has_useful_data and len(overlap) >= 1:
        return True
    
    # ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
    return False

def enrich_scenes_with_captions(facts: List[SceneFacts], captions: List[Dict[str,Any]]) -> List[SceneFacts]:
    """
    ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡πÅ‡∏•‡∏∞ MERGE visual captions ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö scenes ‡∏ï‡∏≤‡∏° timestamp
    - Visual caption ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏£‡∏ß‡∏°‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
    - ‡∏ï‡∏£‡∏ß‡∏à RELEVANCE ‡∏Å‡πà‡∏≠‡∏ô: caption ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö speech ‡∏î‡πâ‡∏ß‡∏¢
    - ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ó‡∏µ‡πà transcript ‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà‡∏°‡∏µ (‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç, ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞)
    """
    for sc in facts:
        # ‡∏´‡∏≤ captions ‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô (¬±2 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ tolerance)
        matched_caps = [
            c for c in captions 
            if c.get("ts") is not None and sc.start - 2.0 <= c["ts"] <= sc.end + 2.0
        ]
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ exact match ‡∏Å‡πá‡∏´‡∏≤ closest 1 ‡∏≠‡∏±‡∏ô (‡∏ñ‡πâ‡∏≤‡∏´‡πà‡∏≤‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 10 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ)
        if not matched_caps and captions:
            sorted_caps = sorted(captions, key=lambda c: abs(c.get("ts", 0) - sc.start))
            closest = sorted_caps[0]
            if abs(closest.get("ts", 0) - sc.start) <= 10.0:
                matched_caps = [closest]
        
        vc, ocrs, tags = [], [], []
        for c in matched_caps:
            cap_text = c.get("caption_detailed") or c.get("caption_short") or ""
            
            # ‚úÖ RELEVANCE CHECK: ‡∏ï‡∏£‡∏ß‡∏à‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡πà‡∏≠‡∏ô merge
            if check_visual_relevance(sc.speech, cap_text):
                vc.append(cap_text)
                if c.get("ocr_text"): 
                    ocrs.append(c["ocr_text"])
                tags.extend(c.get("tags", []))
            else:
                log(f"‚ö†Ô∏è Skipped irrelevant visual: '{cap_text[:50]}...' for speech: '{sc.speech[:50]}...'")
        
        sc.visual_caption = ensure_thai(" ".join(vc).strip()) if vc else ""
        sc.ocr_text = ensure_thai(" ".join(ocrs).strip()) if ocrs else ""
        sc.tags = sorted(list(set(tags)))
    
    return facts

# ====== STEP 5: Visual Evidence (domain-agnostic) ======
WEIGHT_MAP = {
    "number": 3, "percent": 3, "timecode": 3, "date": 3,
    "unit": 2, "currency": 2, "keyword_on_screen": 2,
    "short_len": 1, "has_ocr": 2, "url_or_id": 1, "all_caps_token": 1,
}
PROMO_VAGUE_TERMS = re.compile(
    r"(‡πÇ‡∏õ‡∏£‡πÇ‡∏°‡∏ó|‡∏¢‡∏≠‡∏î‡∏ô‡∏¥‡∏¢‡∏°|‡∏™‡∏∏‡∏î‡∏¢‡∏≠‡∏î|‡∏≠‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£|‡∏ï‡∏∑‡πà‡∏ô‡πÄ‡∏ï‡πâ‡∏ô|‡∏°‡∏±‡∏ô‡∏™‡πå|‡πÄ‡∏à‡πã‡∏á‡∏°‡∏≤‡∏Å|‡∏´‡πâ‡∏≤‡∏°‡∏û‡∏•‡∏≤‡∏î|"
    r"amazing|awesome|must[- ]see|incredible|epic|promo|trailer|official)",
    flags=re.I
)
RE_NUMBER   = re.compile(r"\d")
RE_PERCENT  = re.compile(r"\d+\s*%")
RE_TIMECODE = re.compile(r"\b(?:\d{1,2}:){1,2}\d{2}\b")
RE_DATE     = re.compile(r"\b(?:\d{4}[-/]\d{1,2}[-/]\d{1,2}|(?:\d{1,2}[-/]){1,2}\d{2,4})\b")
RE_UNIT     = re.compile(r"\b(?:kg|g|km|m|cm|mm|mb|gb|tb|fps|hz|px|ms|s|min|hr|‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á|‡∏ô‡∏≤‡∏ó‡∏µ|‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ|‡∏ö‡∏≤‡∏ó|‡∏Å‡∏Å|‡∏Å‡∏°)\b", re.I)
RE_CURRENCY = re.compile(r"[‚Ç¨¬£$¬•‡∏ø]|(?:USD|THB|JPY|EUR)\b", re.I)
RE_URL_ID   = re.compile(r"(https?://\S+)|\b[A-Z0-9]{6,}\b")
RE_ALLCAPS  = re.compile(r"\b[A-Z]{2,}\b")
RE_HEADERY  = re.compile(r"\b(introduction|overview|summary|‡∏™‡∏£‡∏∏‡∏õ|‡∏ö‡∏ó‡∏ô‡∏≥|‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠|‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç|‡∏™‡πÑ‡∏•‡∏î‡πå|‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå|‡∏ß‡∏¥‡∏ò‡∏µ‡πÉ‡∏ä‡πâ|‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏ó‡∏£‡∏≤‡∏ö)\b", re.I)

def _detect_signals(txt: str, has_ocr: bool) -> Dict[str, bool]:
    return {
        "number": bool(RE_NUMBER.search(txt)),
        "percent": bool(RE_PERCENT.search(txt)),
        "timecode": bool(RE_TIMECODE.search(txt)),
        "date": bool(RE_DATE.search(txt)),
        "unit": bool(RE_UNIT.search(txt)),
        "currency": bool(RE_CURRENCY.search(txt)),
        "url_or_id": bool(RE_URL_ID.search(txt)),
        "all_caps_token": bool(RE_ALLCAPS.search(txt)),
        "keyword_on_screen": bool(RE_HEADERY.search(txt)),
        "short_len": len(txt) <= 160,
        "has_ocr": has_ocr,
    }

# ====== STEP 6: Global Summaries (Improved: Transcript + Visual) ======
def summarize_article_th(facts: List[SceneFacts],
                         target_min_words: int = None,
                         target_max_words: int = None) -> str:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏à‡∏≤‡∏Å SceneFacts ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏° speech + visual_caption ‡πÑ‡∏ß‡πâ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô
    ‡∏ó‡∏≥‡πÉ‡∏´‡πâ LLM ‡πÄ‡∏´‡πá‡∏ô context ‡∏ß‡πà‡∏≤‡∏†‡∏≤‡∏û‡∏≠‡∏¢‡∏π‡πà‡∏ï‡∏£‡∏á‡πÑ‡∏´‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
    """
    
    # 1) ‡∏™‡∏£‡πâ‡∏≤‡∏á combined context ‡∏à‡∏≤‡∏Å SceneFacts
    combined_segments = []
    total_speech = ""
    for sc in sorted(facts, key=lambda x: x.start):
        segment = sc.speech
        if sc.visual_caption:
            segment += f" (‡∏†‡∏≤‡∏û: {sc.visual_caption})"
        if sc.ocr_text:
            segment += f" (‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ö‡∏ô‡∏à‡∏≠: {sc.ocr_text})"
        combined_segments.append(segment)
        total_speech += sc.speech + " "
    
    combined_context = "\n".join(combined_segments)
    
    # 2) ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß transcript (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì)
    transcript_word_count = word_count_th(total_speech)
    
    # Dynamic target: ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ï‡∏≤‡∏° transcript
    if target_min_words is None or target_max_words is None:
        if transcript_word_count < 800:
            target_min_words = None
            target_max_words = None
            log(f"üìù Transcript: ~{transcript_word_count} words ‚Üí No length limit")
        else:
            target_min_words = 300
            target_max_words = 400
            log(f"üìù Transcript: ~{transcript_word_count} words ‚Üí Target summary: 300-400 words")
    
    # 3) ‡∏ï‡∏±‡∏î context ‡∏ñ‡πâ‡∏≤‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
    if len(combined_context) > 12000:
        head = combined_context[:5000]
        mid = combined_context[len(combined_context)//2-1500: len(combined_context)//2+1500]
        tail = combined_context[-5000:]
        combined_context = f"{head}\n...\n{mid}\n...\n{tail}"

    # 4) ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt
    length_instruction = f"‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì {target_min_words}-{target_max_words} ‡∏Ñ‡∏≥" if target_min_words else "‡∏™‡∏£‡∏∏‡∏õ‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°"
    
    # 5) System prompt - ‡∏¢‡πâ‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏´‡πâ‡∏≤‡∏°/‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏°‡∏≤‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
    ARTICLE_SYSTEM = f"""‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ô‡∏±‡∏Å‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏°‡∏∑‡∏≠‡∏≠‡∏≤‡∏ä‡∏µ‡∏û ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡∏Ñ‡∏•‡∏¥‡∏õ‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠

‡∏Ç‡πâ‡∏≠‡∏´‡πâ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏ï‡∏≤‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ñ‡∏£‡πà‡∏á‡∏Ñ‡∏£‡∏±‡∏î:
1. ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏™‡πà‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠/‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏° (‡πÄ‡∏ä‡πà‡∏ô **‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á...** ‡∏´‡∏£‡∏∑‡∏≠ # ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠)
2. ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏´‡∏ô‡∏≤ (**) ‡∏´‡∏£‡∏∑‡∏≠ markdown ‡πÉ‡∏î‡πÜ
3. ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏ö‡∏π‡∏•‡πÄ‡∏•‡πá‡∏ï/‡πÄ‡∏•‡∏Ç‡∏•‡∏¥‡∏™‡∏ï‡πå
4. ‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏ï‡πà‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
5. ‡∏´‡πâ‡∏≤‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ", "‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ", "‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ", "‡∏Ñ‡∏•‡∏¥‡∏õ‡∏ô‡∏µ‡πâ", "‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏°‡∏≤‡∏î‡∏π", "‡πÉ‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô"
6. ‡∏´‡πâ‡∏≤‡∏°‡∏û‡∏π‡∏î‡∏ñ‡∏∂‡∏á‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ "transcript", "‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ô‡∏µ‡πâ", "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡∏µ‡πâ", "‡∏ú‡∏π‡πâ‡∏û‡∏π‡∏î"
7. ‡∏´‡πâ‡∏≤‡∏°‡πÅ‡∏™‡∏î‡∏á instructions ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏î‡πÜ ‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö

‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:
- ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏Ñ‡∏•‡∏¥‡∏õ‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠ - ‡πÉ‡∏´‡πâ‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏•‡πà‡∏≤‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á
- ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û‡πÄ‡∏Ç‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥
- ‡∏ñ‡πâ‡∏≤‡∏†‡∏≤‡∏û‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç/‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå/‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞ ‡πÉ‡∏´‡πâ‡πÉ‡∏™‡πà‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ "‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û"
- ‡∏Ñ‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç/‡πÄ‡∏ß‡∏•‡∏≤/‡∏à‡∏≥‡∏ô‡∏ß‡∏ô/‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
- ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á ‡πÄ‡∏•‡πà‡∏≤‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ï‡∏£‡∏á‡πÜ
- ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ñ‡∏≥‡∏ú‡∏¥‡∏î‡∏™‡∏∞‡∏Å‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ñ‡∏≠‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á

‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô"""

    # 6) User prompt - ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏™‡∏±‡πâ‡∏ô‡πÜ + ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
    prompt = f"""‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ñ‡∏•‡∏¥‡∏õ‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ {length_instruction}

{combined_context}"""

    # 7) ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM
    raw = ensure_thai(ollama_summarize(prompt, system=ARTICLE_SYSTEM)) or ""
    
    # 7) Post-processing: ‡∏•‡∏ö‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠/markdown/instructions ‡∏ó‡∏µ‡πà LLM ‡∏≠‡∏≤‡∏à‡πÉ‡∏™‡πà‡∏°‡∏≤
    raw = re.sub(r"^#+\s*.+$", "", raw, flags=re.MULTILINE)  # ‡∏•‡∏ö headings
    raw = re.sub(r"\*\*[^*]+\*\*", "", raw)  # ‡∏•‡∏ö bold
    raw = re.sub(r"^\*\*‡∏Ç‡πâ‡∏≠‡∏´‡πâ‡∏≤‡∏°.*$", "", raw, flags=re.MULTILINE)  # ‡∏•‡∏ö instruction lines
    raw = re.sub(r"^\*\*‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î.*$", "", raw, flags=re.MULTILINE)
    raw = re.sub(r"^\d+\.\s*‡∏´‡πâ‡∏≤‡∏°.*$", "", raw, flags=re.MULTILINE)  # ‡∏•‡∏ö numbered prohibitions
    raw = re.sub(r"\[‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤.*?\]", "", raw)  # ‡∏•‡∏ö [‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤...] markers
    raw = re.sub(r"\[\d+[-‚Äì]\d+s?\]", "", raw)  # ‡∏•‡∏ö timestamp brackets ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠
    raw = re.sub(r"\n{3,}", "\n\n", raw)  # ‡∏•‡∏î newlines ‡∏ã‡πâ‡∏≥
    raw = raw.strip()
    
    return raw

def extract_single_keyword_th(text: str) -> str:
    """
    ‡πÉ‡∏ä‡πâ LLM ‡∏™‡∏Å‡∏±‡∏î '‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏´‡∏•‡∏±‡∏Å' ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≥‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢) ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤
    """
    prompt = f"""
‡∏≠‡πà‡∏≤‡∏ô‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ ‡πÅ‡∏•‡πâ‡∏ß‡∏ï‡∏≠‡∏ö‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏Ñ‡∏≥‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô "‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏´‡∏•‡∏±‡∏Å" ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
- ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏õ‡∏£‡∏∞‡∏™‡∏°‡∏¢‡∏≤‡∏ß‡πÜ ‡∏´‡∏£‡∏∑‡∏≠‡∏ß‡∏•‡∏µ (‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏ô‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏Ñ‡∏≥‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
- ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢
- ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡∏≥‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
- ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢

[‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°]
{text}
"""
    out = ollama_summarize(prompt, options={"temperature": 0.0, "num_ctx": 1024})
    # ‡∏ï‡∏±‡∏î‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î/‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏≥‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    words = (out or "").strip().split()
    if not words:
        return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç"
    keyword = words[0]
    keyword = re.sub(r"[^\w‡∏Å-‡πô]", "", keyword)
    return keyword or "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç"

def _safe_word_count(path: str):
    try:
        from pythainlp.tokenize import word_tokenize
        with open(path, "r", encoding="utf-8") as f:
            text = f.read()
            tokens = word_tokenize(text, engine="newmm")
            return len(tokens)
    except Exception:
        return None

# ====== (‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï) MAIN: ‡∏ú‡∏•‡∏¥‡∏ï‡πÄ‡∏â‡∏û‡∏≤‡∏∞ transcript+visual ======
def main():
    t0 = time.time()
    for c in ["ffmpeg", "ffprobe"]:
        check_cmd(c)

    transcript = None
    segments = None
    duration = None
    used_youtube_api = False
    
    # 2) Transcript - ‡∏•‡∏≠‡∏á YouTube Transcript API ‡∏Å‡πà‡∏≠‡∏ô (‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡∏¥‡∏î)
    if USE_YOUTUBE_TRANSCRIPT:
        try:
            log("üìù ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ YouTube Transcript API...")
            transcript, segments = transcribe_youtube_api(YOUTUBE_URL, LANGUAGE, step_start=10, step_end=45)
            used_youtube_api = True
            log("‚úÖ ‡πÉ‡∏ä‡πâ YouTube Transcript API ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
        except Exception as e:
            log(f"‚ö†Ô∏è YouTube Transcript API ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")
            log("üîÑ Fallback ‡πÑ‡∏õ‡πÉ‡∏ä‡πâ Whisper...")
    
    # ‡∏ñ‡πâ‡∏≤ YouTube API ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ -> ‡πÉ‡∏ä‡πâ Whisper
    if transcript is None:
        # 1) ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á + ‡∏ï‡∏±‡∏î‡∏â‡∏≤‡∏Å
        download_audio_wav_16k(YOUTUBE_URL, AUDIO_OUT)
        download_t = time.time()
        download_time = download_t - t0
        send_progress("‡πÇ‡∏´‡∏•‡∏î‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠", 10, 100)

        # Transcript ‡∏î‡πâ‡∏ß‡∏¢ Whisper
        transcript, segments = transcribe_whisper(AUDIO_OUT,WHISPER_MODEL,LANGUAGE,ASR_DEVICE,step_start=10,step_end=45)
    else:
        send_progress("‡πÇ‡∏´‡∏•‡∏î‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠", 10, 100)
        send_progress("‡∏ñ‡∏≠‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á", 45, 100)
        download_time = 0 
    
    asr_t = time.time()
    asr_time = asr_t - t0 if not used_youtube_api else 0
    send_progress("‡∏ñ‡∏≠‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á", 45, 100)

    # 3) Caption + OCR
    captioner = VisionCaptioner(VL_MODEL_NAME, VL_DEVICE)
    duration = stream_scene_frames_and_caption(YOUTUBE_URL, FRAMES_DIR, SCENE_THRESH, CAPTIONS_JSON, captioner, video_duration=duration)
    with open(CAPTIONS_JSON, "r", encoding="utf-8") as f:
        caps = json.load(f)
    scene_ts = [c["ts"] for c in caps if "ts" in c]
    # with open(SCENES_JSON, "w", encoding="utf-8") as f:
    #     json.dump(scene_ts, f, ensure_ascii=False, indent=2)

    # ‡πÉ‡∏ä‡πâ split_segments_to_scenes ‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡∏ï‡∏≤‡∏° timestamp ‡∏à‡∏£‡∏¥‡∏á
    facts = split_segments_to_scenes(segments, scene_ts)
    facts = enrich_scenes_with_captions(facts, caps)
    frames_count = len(caps)
    with open(SCENE_FACTS_JSON, "w", encoding="utf-8") as f:
        json.dump([asdict(x) for x in facts], f, ensure_ascii=False, indent=2)
    log(f"‚úÖ Scene facts saved -> {SCENE_FACTS_JSON}")
    cap_t = time.time()
    cap_time = cap_t - asr_t
    send_progress("‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡∏†‡∏≤‡∏û", 80, 100)

    article_th = summarize_article_th(facts)
    send_progress("‡∏ó‡∏≥‡∏™‡∏£‡∏∏‡∏õ", 90, 67)
    
    with open(FINAL_ARTICLE_TXT, "w", encoding="utf-8") as f:
        f.write(wrap_text(article_th))
        
    main_keyword = extract_single_keyword_th(article_th)
    send_progress("‡∏ó‡∏≥‡∏™‡∏£‡∏∏‡∏õ", 95, 100)
    summarize_t = time.time()
    summarize_time = summarize_t - cap_t

    log("\n===== SHORT ARTICLE (TH) =====")
    log(article_th or "(empty)")

    t1 = time.time()
    log("\n‚úÖ Done.")
    log(f"\n‚è±Ô∏è Elapsed: {t1 - t0:.2f} sec")
    
    log(f"""TIME BREAKDOWN:
          - Download & Audio Extract: {download_time:.2f} sec
          - ASR (Whisper): {asr_time:.2f} sec
          - Captioning + OCR: {cap_time:.2f} sec
          - Summarization: {summarize_time:.2f} sec""")
    
        # === LOG TO EXCEL ===
    try:
        scenes_count = len(scene_ts) if isinstance(scene_ts, list) else None
        captions_count = len(caps) if isinstance(caps, list) else None
        bullets_count = 0  # ‡πÑ‡∏°‡πà‡∏°‡∏µ items ‡πÅ‡∏•‡πâ‡∏ß
        article_words = _safe_word_count(FINAL_ARTICLE_TXT)
        transcript_words = _safe_word_count(TRANSCRIPT_TXT)

        row = {
            "timestamp": datetime.now().isoformat(timespec="seconds"),
            # ---- CONFIG / PARAMS ----
            "youtube_url": YOUTUBE_URL,
            "whisper_model": WHISPER_MODEL,
            "whisper_temp": WHISPER_TEMP,
            "asr_device": ASR_DEVICE,
            "vl_device": VL_DEVICE,
            "vl_model": VL_MODEL_NAME,
            "scene_thresh": SCENE_THRESH,
            "enable_ocr": ENABLE_OCR,
            # ---- COUNTS / SIZES ----
            "frames": frames_count,
            "scenes": scenes_count,
            "captions": captions_count,
            "bullets": bullets_count,
            "transcript_words": transcript_words,
            "article_words": article_words,
            "keyword": main_keyword,
            # ---- OUTPUT FILES ----
            # "audio_out": AUDIO_OUT,
            # "scenes_json": SCENES_JSON,
            # "captions_json": CAPTIONS_JSON,
            # "scene_facts_json": SCENE_FACTS_JSON,
            # "dropdown_json": DROPDOWN_JSON,
            # "final_bullets_txt": FINAL_TXT,
            # "final_article_txt": FINAL_ARTICLE_TXT,
            # ---- TIMING (sec) ----
            "t_download": round(download_time, 2),
            "t_asr": round(asr_time, 2),
            "t_caption": round(cap_time, 2),
            "t_summarize": round(summarize_time, 2),
            "t_total": None,  # ‡∏à‡∏∞‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á
            "duration_sec": round(duration, 2) if isinstance(duration, (int, float)) else None,
        }
        
        log(" keyword: " + main_keyword)

        # ‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏ß‡∏° (‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ t0/t1 ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß)
        try:
            row["t_total"] = round(time.time() - t0, 2)
        except Exception:
            pass

        try:
            if METRICS_JSON:
                os.makedirs(os.path.dirname(METRICS_JSON), exist_ok=True)
                import json as _json
                with open(METRICS_JSON, "w", encoding="utf-8") as f:
                    _json.dump(row, f, ensure_ascii=False)
                log(f"üìù Metrics saved -> {METRICS_JSON}")
            else:
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î METRICS_JSON ‡πÑ‡∏ß‡πâ ‡∏Å‡πá‡∏Ç‡πâ‡∏≤‡∏°‡πÄ‡∏â‡∏¢‡πÜ
                pass
        except Exception as e:
            log(f"‚ö†Ô∏è Metrics JSON write failed: {e}")
    except Exception as e:
        log(f"‚ö†Ô∏è Statistic logging failed: {e}")
    send_progress("‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", 99, 80)


if __name__ == "__main__":
    main()
