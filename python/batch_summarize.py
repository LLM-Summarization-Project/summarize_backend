"""
Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏±‡∏ô YouTube URLs ‡∏ó‡∏µ‡∏•‡∏∞ row ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
‡∏£‡∏±‡∏ô 4 ‡∏£‡∏≠‡∏ö‡∏ï‡πà‡∏≠ URL ‡πÇ‡∏î‡∏¢‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô WHISPER_TEMP ‡πÉ‡∏ô .env ‡πÅ‡∏•‡πâ‡∏ß restart worker
‡πÉ‡∏ä‡πâ: python batch_summarize.py urls.txt [--start N]
"""
import sys
import time
import subprocess
import requests
from pathlib import Path
import os
import argparse
from datetime import datetime

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ API
API_BASE = "http://localhost:8081"  # Backend URL
AUTH_BASE = "http://localhost:4005"  # Auth service URL
TEST_USER_ID = 2
SYSTEM_PROMPT_TH = (
    "‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏•‡πà‡∏≤‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏¢‡πà‡∏≠‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏ö‡∏π‡∏•‡πÄ‡∏•‡πá‡∏ï "
    "‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏π‡∏î‡∏ã‡πâ‡∏≥ ‡πÅ‡∏•‡∏∞‡∏´‡∏≤‡∏Å‡∏≠‡πâ‡∏≤‡∏á‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏†‡∏≤‡∏û‡πÉ‡∏´‡πâ‡πÅ‡∏ó‡∏£‡∏Å (‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û) ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ "
    "‡∏Ñ‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞/‡∏ï‡∏±‡∏ß‡∏¢‡πà‡∏≠/‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö"
)


# ‚ö†Ô∏è WHISPER_TEMP ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö
WHISPER_TEMPS = [0.0, 0.2, 0.4, 0.6]

# Path to .env file
ENV_FILE = Path(__file__).parent.parent / ".env"

# Path to logs directory
LOGS_DIR = Path(__file__).parent / "logs"

# Global log file handle
LOG_FILE = None

# ‚ö†Ô∏è ‡πÉ‡∏™‡πà refresh token ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
REFRESH_TOKEN = os.getenv("REFRESH_TOKEN", "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOjYsImlhdCI6MTc2NzczNTc3MCwiZXhwIjoxNzY4MzQwNTcwfQ.eeJjnK1B9Md4NXFyVcWQGzC6LkmTpObCaDMVpQ87ZQU")

# Access token (‡∏à‡∏∞ refresh ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥)
ACCESS_TOKEN = ""

def log(message: str, end: str = "\n"):
    """Print and log a message"""
    print(message, end=end)
    if LOG_FILE:
        LOG_FILE.write(message + (end if end != "\r" else "\n"))
        LOG_FILE.flush()

def setup_logging():
    """Setup log file for this session"""
    global LOG_FILE
    LOGS_DIR.mkdir(exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_path = LOGS_DIR / f"batch_{timestamp}.log"
    LOG_FILE = open(log_path, "w", encoding="utf-8")
    log(f"üìù Logging to: {log_path}")
    return log_path

def refresh_access_token():
    """‡∏î‡∏∂‡∏á access token ‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å refresh token"""
    global ACCESS_TOKEN
    
    if not REFRESH_TOKEN:
        log("‚ö†Ô∏è No REFRESH_TOKEN set, cannot refresh")
        return False
    
    try:
        resp = requests.post(
            f"{AUTH_BASE}/auth/refresh",
            cookies={"refresh_token": REFRESH_TOKEN}  # ‡∏™‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô cookie ‡πÅ‡∏ó‡∏ô
        )
        
        if resp.status_code == 200 or resp.status_code == 201:
            # Access token ‡∏°‡∏≤‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö cookie, ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà JSON body
            ACCESS_TOKEN = resp.cookies.get("access_token", "")
            if not ACCESS_TOKEN:
                # Fallback: ‡∏•‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å JSON body
                data = resp.json()
                ACCESS_TOKEN = data.get("accessToken") or data.get("access_token", "")
            log(f"üîÑ Token refreshed successfully")
            return True
        else:
            log(f"‚ùå Refresh failed: {resp.status_code} - {resp.text}")
            return False
            
    except Exception as e:
        log(f"‚ùå Refresh error: {e}")
        return False

def get_headers():
    """‡∏™‡∏£‡πâ‡∏≤‡∏á headers ‡∏û‡∏£‡πâ‡∏≠‡∏° token"""
    if ACCESS_TOKEN:
        return {"Authorization": f"Bearer {ACCESS_TOKEN}"}
    return {"x-user-id": str(TEST_USER_ID)}

def update_env_whisper_temp(temp: float):
    """‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó WHISPER_TEMP ‡πÉ‡∏ô .env file"""
    if not ENV_FILE.exists():
        log(f"‚ùå .env file not found: {ENV_FILE}")
        return False
    
    lines = ENV_FILE.read_text(encoding="utf-8").splitlines()
    new_lines = []
    found = False
    
    for line in lines:
        if line.startswith("WHISPER_TEMP="):
            new_lines.append(f"WHISPER_TEMP={temp}")
            found = True
        else:
            new_lines.append(line)
    
    if not found:
        new_lines.append(f"WHISPER_TEMP={temp}")
    
    ENV_FILE.write_text("\n".join(new_lines) + "\n", encoding="utf-8")
    log(f"   üìù Updated .env: WHISPER_TEMP={temp}")
    return True

def wait_for_completion(job_id: str, timeout: int = 1200) -> dict:  # 20 minutes
    """‡∏£‡∏≠‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤ job ‡∏à‡∏∞‡πÄ‡∏™‡∏£‡πá‡∏à"""
    start = time.time()
    while time.time() - start < timeout:
        try:
            resp = requests.get(f"{API_BASE}/summary/{job_id}", headers=get_headers())
            
            if resp.status_code == 401:
                if refresh_access_token():
                    continue
                    
            if resp.status_code == 200:
                data = resp.json()
                status = data.get("status", "")
                
                if status == "DONE":
                    return {"status": "success", "data": data}
                elif status in ["FAILED", "ERROR", "CANCEL"]:
                    return {"status": "failed", "error": status}
                
                percent = data.get("percent", 0)
                log(f"   ‚è≥ {status} - {percent}%", end="\r")
                
        except Exception as e:
            log(f"   ‚ö†Ô∏è Error checking status: {e}")
        
        time.sleep(3)
    
    return {"status": "timeout"}

def submit_job(url: str, temp: float = None) -> dict:
    """‡∏™‡πà‡∏á job ‡πÑ‡∏õ queue ‡∏û‡∏£‡πâ‡∏≠‡∏° whisperTemp"""
    try:
        payload = {"youtubeUrl": url}
        if temp is not None:
            payload["whisperTemp"] = temp
            
        resp = requests.post(
            f"{API_BASE}/summary",
            json=payload,
            headers=get_headers()
        )
        
        if resp.status_code == 401:
            if refresh_access_token():
                resp = requests.post(
                    f"{API_BASE}/summary",
                    json=payload,
                    headers=get_headers()
                )
        
        if resp.status_code in [200, 201]:
            return resp.json()
        else:
            return {"error": f"HTTP {resp.status_code}: {resp.text}"}
            
    except Exception as e:
        return {"error": str(e)}

def run_batch(url_file: str, start_job: int = 1):
    """‡∏£‡∏±‡∏ô summarization ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ URL, 4 ‡∏£‡∏≠‡∏ö‡∏ï‡πà‡∏≠ URL
    
    Args:
        url_file: Path to file containing URLs
        start_job: Job number to start from (1-indexed)
    """
    
    # Setup logging
    log_path = setup_logging()
    
    # ‡∏≠‡πà‡∏≤‡∏ô URLs ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå
    url_path = Path(url_file)
    if not url_path.exists():
        log(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {url_file}")
        return
    
    urls = [line.strip() for line in url_path.read_text(encoding="utf-8").splitlines() 
            if line.strip() and not line.startswith("#")]
    
    if not urls:
        log("‚ùå ‡πÑ‡∏°‡πà‡∏°‡∏µ URLs ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå")
        return
    
    total_jobs = len(urls) * len(WHISPER_TEMPS)
    log(f"üìã ‡∏û‡∏ö {len(urls)} URLs √ó {len(WHISPER_TEMPS)} temps = {total_jobs} jobs")
    log(f"üå°Ô∏è WHISPER_TEMPS: {WHISPER_TEMPS}")
    log(f"üîó API: {API_BASE}")
    
    if start_job > 1:
        log(f"‚è© Starting from job #{start_job}")
    
    log("=" * 60)
    
    # Refresh token ‡∏Å‡πà‡∏≠‡∏ô
    if REFRESH_TOKEN:
        log("üîÑ Refreshing token...")
        refresh_access_token()
    
    results = []
    job_num = 0
    
    for temp in WHISPER_TEMPS:
        log(f"\n{'='*60}")
        log(f"üå°Ô∏è ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏£‡∏≠‡∏ö WHISPER_TEMP = {temp}")
        log(f"{'='*60}")
        
        for url in urls:
            job_num += 1
            
            # Skip jobs before start_job
            if job_num < start_job:
                log(f"\n[{job_num}/{total_jobs}] ‚è≠Ô∏è Skipping (before start_job={start_job})")
                continue
            
            log(f"\n[{job_num}/{total_jobs}] üé¨ {url}")
            log(f"   üå°Ô∏è WHISPER_TEMP = {temp}")
            start_time = time.time()
            
            # 1) ‡∏™‡πà‡∏á job ‡∏û‡∏£‡πâ‡∏≠‡∏° whisperTemp ‡πÉ‡∏ô request body
            log("   üì§ Submitting job...")
            submit_result = submit_job(url, temp)
            
            if "error" in submit_result:
                log(f"   ‚ùå Submit failed: {submit_result['error']}")
                results.append({
                    "url": url, 
                    "temp": temp,
                    "status": "submit_failed", 
                    "time": 0
                })
                continue
            
            job_id = submit_result.get("jobId")
            from_cache = submit_result.get("fromCache", False)
            
            if from_cache:
                log(f"   üì¶ From cache: {job_id}")
                results.append({
                    "url": url, 
                    "temp": temp,
                    "status": "cached", 
                    "time": 0, 
                    "job_id": job_id
                })
                continue
            
            log(f"   üì• Job ID: {job_id}")
            
            # 2) ‡∏£‡∏≠‡∏à‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à
            log("   ‚è≥ Waiting for completion...")
            wait_result = wait_for_completion(job_id)
            elapsed = time.time() - start_time
            
            if wait_result["status"] == "success":
                log(f"   ‚úÖ Done! ({elapsed:.1f}s)                    ")
                results.append({
                    "url": url, 
                    "temp": temp,
                    "status": "success", 
                    "time": elapsed, 
                    "job_id": job_id
                })
            else:
                log(f"   ‚ùå {wait_result['status']} ({elapsed:.1f}s)")
                results.append({
                    "url": url, 
                    "temp": temp,
                    "status": wait_result["status"], 
                    "time": elapsed, 
                    "job_id": job_id
                })
    
    # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
    log("\n" + "=" * 60)
    log("üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•:")
    success = sum(1 for r in results if r["status"] in ["success", "cached"])
    failed = len(results) - success
    total_time = sum(r["time"] for r in results)
    
    log(f"   ‚úÖ ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {success}/{len(results)}")
    log(f"   ‚ùå ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {failed}/{len(results)}")
    log(f"   ‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏ß‡∏°: {total_time:.1f}s")
    
    if start_job > 1:
        log(f"   ‚è© (started from job #{start_job})")
    
    # ‡∏™‡∏£‡∏∏‡∏õ‡∏ï‡∏≤‡∏° temp
    log("\nüìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ï‡∏≤‡∏° WHISPER_TEMP:")
    for temp in WHISPER_TEMPS:
        temp_results = [r for r in results if r["temp"] == temp]
        temp_success = sum(1 for r in temp_results if r["status"] in ["success", "cached"])
        log(f"   üå°Ô∏è {temp}: {temp_success}/{len(temp_results)} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
    
    if failed > 0:
        log("\n‚ùå Jobs ‡∏ó‡∏µ‡πà‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß:")
        for r in results:
            if r["status"] not in ["success", "cached"]:
                log(f"   - {r['url']} (temp={r['temp']}) - {r['status']}")
    
    # Close log file
    if LOG_FILE:
        log(f"\nüìù Log saved to: {log_path}")
        LOG_FILE.close()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Batch YouTube summarization script",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=f"""
‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤:
  API_BASE = {API_BASE}
  WHISPER_TEMPS = {WHISPER_TEMPS}
  ENV_FILE = {ENV_FILE}
  LOGS_DIR = {LOGS_DIR}
"""
    )
    parser.add_argument("urls_file", help="‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ YouTube URLs ‡∏ó‡∏µ‡∏•‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î")
    parser.add_argument(
        "--start", "-s", 
        type=int, 
        default=1,
        metavar="N",
        help="Start from job number N (1-indexed, default: 1)"
    )
    
    args = parser.parse_args()
    
    if args.start < 1:
        print("‚ùå --start must be >= 1")
        sys.exit(1)
    
    run_batch(args.urls_file, args.start)
